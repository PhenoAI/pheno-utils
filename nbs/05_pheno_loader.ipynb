{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Class for loading datasets on the research platform\n",
    "output-file: pheno_loader.html\n",
    "title: Pheno loader\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp pheno_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from glob import glob\n",
    "import traceback\n",
    "\n",
    "import os\n",
    "import re\n",
    "from typing import List, Any, Dict, Union\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from pheno_utils.config import (\n",
    "    DATASETS_PATH,\n",
    "    COHORT, \n",
    "    EVENTS_DATASET, \n",
    "    ERROR_ACTION, \n",
    "    BULK_DATA_PATH,\n",
    "    DICT_PROPERTY_PATH, \n",
    "    DATA_CODING_PATH,\n",
    "    PREFERRED_LANGUAGE\n",
    "    )\n",
    "from pheno_utils.basic_analysis import custom_describe\n",
    "from pheno_utils.bulk_data_loader import get_function_for_field_type\n",
    "from pheno_utils.questionnaires_handler import transform_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class PhenoLoader:\n",
    "    \"\"\"\n",
    "    Class to load multiple tables from a dataset and allows to easily access\n",
    "    their fields.\n",
    "\n",
    "    Args:\n",
    "    \n",
    "        dataset (str): The name of the dataset to load.\n",
    "        base_path (str, optional): The base path where the data is stored. Defaults to DATASETS_PATH.\n",
    "        cohort (str, optional): The name of the cohort within the dataset. Defaults to COHORT.\n",
    "        age_sex_dataset (str, optional): The name of the dataset to use for computing age and sex. Defaults to EVENTS_DATASET.\n",
    "        skip_dfs (list, optional): A list of tables (or substrings that match to tables) to skip when loading the data. Defaults to [].\n",
    "        unique_index (bool, optional): Whether to ensure the index of the data is unique. Defaults to False.\n",
    "        valid_dates (bool, optional): Whether to ensure that all timestamps in the data are valid dates. Defaults to False.\n",
    "        valid_stage (bool, optional): Whether to ensure that all research stages in the data are valid. Defaults to False.\n",
    "        flexible_field_search (bool, optional): Whether to allow regex field search. Defaults to False.\n",
    "        keep_undefined_research_stage (bool, optional): Whether to keep samples with undefined research stage. Defaults to False.\n",
    "        errors (str, optional): Whether to raise an error or issue a warning if missing data is encountered.\n",
    "            Possible values are 'raise', 'warn' and 'ignore'. Defaults to ERROR_ACTION.\n",
    "\n",
    "    Attributes:\n",
    "    \n",
    "        dict (pd.DataFrame): The data dictionary for the dataset, containing information about each field.\n",
    "        dfs (dict): A dictionary of dataframes, one for each table in the dataset.\n",
    "        fields (list): A list of all fields in the dataset.\n",
    "        dataset (str): The name of the dataset being used.\n",
    "        cohort (str): The name of the cohort being used.\n",
    "        base_path (str): The base path where the data is stored.\n",
    "        dataset_path (str): The full path to the dataset being used.\n",
    "        age_sex_dataset (str): The name of the dataset being used to compute age and sex.\n",
    "        skip_dfs (list): A list of tables to skip when loading the data.\n",
    "        unique_index (bool): Whether to ensure the index of the data is unique.\n",
    "        valid_dates (bool): Whether to ensure that all timestamps in the data are valid dates.\n",
    "        valid_stage (bool): Whether to ensure that all research stages in the data are valid.\n",
    "        flexible_field_search (bool): Whether to allow regex field search.\n",
    "        keep_undefined_research_stage (bool, optional): Whether to keep samples with undefined research stage.\n",
    "        errors (str): Whether to raise an error or issue a warning if missing data is encountered.\n",
    "        preferred_language (str): The preferred language for the questionnaires.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: str,\n",
    "        base_path: str = DATASETS_PATH,\n",
    "        cohort: str = COHORT,\n",
    "        age_sex_dataset: str = EVENTS_DATASET,\n",
    "        skip_dfs: List[str] = [],\n",
    "        unique_index: bool = False,\n",
    "        valid_dates: bool = False,\n",
    "        valid_stage: bool = False,\n",
    "        flexible_field_search: bool = False,\n",
    "        squeeze: bool = False,\n",
    "        errors: str = ERROR_ACTION,\n",
    "        read_parquet_kwargs: Dict[str, Any] = {},\n",
    "        preferred_language: str = PREFERRED_LANGUAGE,\n",
    "        keep_undefined_research_stage: bool = False\n",
    "    ) -> None:\n",
    "        self.dataset = dataset\n",
    "        self.cohort = cohort\n",
    "        self.base_path = base_path\n",
    "        self.dataset_path = self.__get_dataset_path__(self.dataset)\n",
    "        if self.dataset not in [age_sex_dataset, 'population']:\n",
    "            self.age_sex_dataset = age_sex_dataset\n",
    "        else:\n",
    "            self.age_sex_dataset = None\n",
    "        self.skip_dfs = skip_dfs\n",
    "        self.unique_index = unique_index\n",
    "        self.valid_dates = valid_dates\n",
    "        self.valid_stage = valid_stage\n",
    "        self.flexible_field_search = flexible_field_search\n",
    "        self.squeeze = squeeze\n",
    "        self.errors = errors\n",
    "        self.read_parquet_kwargs = read_parquet_kwargs\n",
    "        self.preferred_language = preferred_language\n",
    "        self.data_codings = pd.read_csv(DATA_CODING_PATH) # TODO: convert to csv when it will be available\n",
    "        self.__load_dictionary__()\n",
    "        self.__load_dataframes__()\n",
    "        if self.age_sex_dataset is not None:\n",
    "            self.__load_age_sex__()\n",
    "        self.dict_prop = pd.read_csv(DICT_PROPERTY_PATH, index_col='field_type')\n",
    "        self.keep_undefined_research_stage = keep_undefined_research_stage\n",
    "\n",
    "    def load_sample_data(\n",
    "        self,\n",
    "        field_name: Union[str, List[str]],\n",
    "        participant_id: Union[None, int, List[int]] = None,\n",
    "        research_stage: Union[None, str, List[str]] = None,\n",
    "        array_index: Union[None, int, List[int]] = None,\n",
    "        parent_bulk: Union[None, str] = None,\n",
    "        load_func: callable = None,\n",
    "        concat: bool = True,\n",
    "        pivot=None, \n",
    "        keep_undefined_research_stage: Union[None, str] = None,\n",
    "        **kwargs\n",
    "    ) -> Union[pd.DataFrame, None]:\n",
    "        \"\"\"\n",
    "        Load time series or bulk data for sample(s).\n",
    "        Deprecated function. See load_bulk_data().\n",
    "        \"\"\"\n",
    "        warnings.warn('load_sample_data() is deprecated in favour of load_bulk_data() and will be removed in a future version.')\n",
    "        return self.load_bulk_data(field_name,\n",
    "                                   participant_id=participant_id,\n",
    "                                   research_stage=research_stage,\n",
    "                                   array_index=array_index,\n",
    "                                   parent_bulk=parent_bulk,\n",
    "                                   load_func=load_func,\n",
    "                                   concat=concat,\n",
    "                                   pivot=pivot,\n",
    "                                   keep_undefined_research_stage=keep_undefined_research_stage,\n",
    "                                   **kwargs)\n",
    "\n",
    "    def load_bulk_data(\n",
    "        self,\n",
    "        field_name: Union[str, List[str]],\n",
    "        participant_id: Union[None, int, List[int]] = None,\n",
    "        research_stage: Union[None, str, List[str]] = None,\n",
    "        array_index: Union[None, int, List[int]] = None,\n",
    "        parent_bulk: Union[None, str] = None,\n",
    "        load_func: callable = None,\n",
    "        concat: bool = True,\n",
    "        pivot=None,\n",
    "        keep_undefined_research_stage: Union[None, str] = None,\n",
    "        **kwargs\n",
    "    ) -> Union[pd.DataFrame, None]:\n",
    "        \"\"\"\n",
    "        Load time series or bulk data for sample(s).\n",
    "\n",
    "        Args:\n",
    "            field_name (str or List): The name of the field(s) to load.\n",
    "            parent_bulk (str, optional): The name of the field that points to the bulk data file. Defaults to None (inferred from field_name).\n",
    "            participant_id (str or list, optional): The participant ID or IDs to load data for.\n",
    "            research_stage (str or list, optional): The research stage or stages to load data for.\n",
    "            array_index (int or list, optional): The array index or indices to load data for.\n",
    "            load_func (callable, optional): [Deprecated] The function to use to load the data. Defaults to pd.read_parquet\n",
    "            concat (bool, optional): Whether to concatenate the data into a single DataFrame. Automatically ignored if data is not a DataFrame. Defaults to True.\n",
    "            pivot (str, optional): The name of the field to pivot the data on (if DataFrame). Defaults to None.\n",
    "            keep_undefined_research_stage (bool, optional): Whether to keep samples with undefined research stage. Defaults to None.\n",
    "        \"\"\"\n",
    "        if keep_undefined_research_stage is None:\n",
    "            keep_undefined_research_stage = self.keep_undefined_research_stage\n",
    "        # get path to bulk file\n",
    "        if type(field_name) is str:\n",
    "            field_name = [field_name]\n",
    "        sample, fields = self.get(field_name + ['participant_id'], return_fields=True, keep_undefined_research_stage=keep_undefined_research_stage)\n",
    "        fields = [f for f in fields if f != 'participant_id']  # these are fields, as opposed to parent_bulk\n",
    "        # TODO: slice bulk data based on field_type\n",
    "        if sample.shape[1] > 2:\n",
    "            if parent_bulk is not None:\n",
    "                # get the field_name associated with parent_bulk\n",
    "                # sample = sample[[parent_bulk, 'participant_id']]\n",
    "                sample = sample.get([parent_bulk, 'participant_id'], keep_undefined_research_stage=keep_undefined_research_stage)\n",
    "            else:\n",
    "                if self.errors == 'raise':\n",
    "                    raise ValueError(f'More than one field found for {field_name}. Specify parent_bulk')\n",
    "                elif self.errors == 'warn':\n",
    "                    warnings.warn(f'More than one field found for {field_name}. Specify parent_bulk')\n",
    "        col = sample.columns.drop('participant_id')[0]  # can be different from field_name if parent_dataframe is implied\n",
    "        sample = sample.astype({col: str})\n",
    "\n",
    "        # filter by participant_id, research_stage and array_index\n",
    "        query_str = []\n",
    "        if participant_id is not None:\n",
    "            query_str.append('participant_id in @participant_id')\n",
    "            if not isinstance(participant_id, list):\n",
    "                participant_id = [participant_id]\n",
    "        if research_stage is not None:\n",
    "            if not isinstance(research_stage, list):\n",
    "                research_stage = [research_stage]\n",
    "            query_str.append('research_stage in @research_stage')\n",
    "        if array_index is not None:\n",
    "            if not isinstance(array_index, list):\n",
    "                array_index = [array_index]\n",
    "            query_str.append('array_index in @array_index')\n",
    "        query_str = ' and '.join(query_str)\n",
    "        if query_str:\n",
    "            sample = sample.query(query_str)\n",
    "\n",
    "        # check for missing samples\n",
    "        if participant_id is not None:\n",
    "            missing_participants = np.setdiff1d(participant_id, sample['participant_id'].unique())\n",
    "        else:\n",
    "            missing_participants = []\n",
    "\n",
    "        if len(missing_participants):\n",
    "            if self.errors == 'raise':\n",
    "                raise ValueError(f'Missing samples: {missing_participants}')\n",
    "            elif self.errors == 'warn':\n",
    "                warnings.warn(f'Missing samples: {missing_participants}')\n",
    "        if len(sample) == 0:\n",
    "            return None\n",
    "\n",
    "        # load data\n",
    "        if load_func is not None:\n",
    "            warnings.warn(\"The 'load_func' is deprecated and will be removed in future versions.\")\n",
    "        else: \n",
    "            if 'field_type' not in self.dict:\n",
    "                field_type = None\n",
    "            else:\n",
    "                field_type = self.dict.loc[field_name, 'field_type'].values[0]\n",
    "            load_func = get_function_for_field_type(field_type)\n",
    "        sample = sample.loc[:, col]\n",
    "        sample = self.__slice_bulk_partition__(fields, sample)\n",
    "        kwargs.update(self.__slice_bulk_data__(fields))\n",
    "        data = []\n",
    "        for p in sample.unique():\n",
    "            try:\n",
    "                data.append(load_func(p, **kwargs))\n",
    "                if isinstance(data[-1], pd.DataFrame):\n",
    "                    data[-1] = self.__add_missing_levels__(data[-1], sample.loc[sample == p].to_frame())\n",
    "                    if query_str:\n",
    "                        data[-1] = data[-1].query(query_str)\n",
    "                    data[-1].sort_index(inplace=True)\n",
    "            except Exception as e:\n",
    "                if self.errors == 'raise':\n",
    "                    raise e\n",
    "                elif self.errors == 'warn':\n",
    "                    warnings.warn(f'Error loading {p}: {e}')\n",
    "\n",
    "        # format the final result\n",
    "        if concat and isinstance(data[0], pd.DataFrame):\n",
    "            data = pd.concat(data, axis=0)\n",
    "        if pivot is not None and isinstance(data, pd.DataFrame):\n",
    "            values = data.columns\n",
    "            if len(values) == 1:\n",
    "                values = values[0]\n",
    "            if pivot in data.index.names:\n",
    "                data = data.reset_index(pivot)\n",
    "            data = data.pivot(columns=pivot, values=values)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Return string representation of object\n",
    "\n",
    "        Returns:\n",
    "            str: String representation of object\n",
    "        \"\"\"\n",
    "        return self.__str__()\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Return string representation of object\n",
    "\n",
    "        Returns:\n",
    "            str: String representation of object\n",
    "        \"\"\"\n",
    "        return f'PhenoLoader for {self.dataset} with' +\\\n",
    "            f'\\n{len(self.fields)} fields\\n{len(self.dfs)} tables: {list(self.dfs.keys())}'\n",
    "\n",
    "    def __getitem__(self, fields: Union[str,List[str]]):\n",
    "        \"\"\"\n",
    "        Return data for the specified fields from all tables\n",
    "\n",
    "        Args:\n",
    "            fields (Union[str, List[str]]): Fields to return\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Data for the specified fields from all tables\n",
    "        \"\"\"\n",
    "        return self.get(fields)\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_indices_overlap(df1, df2):\n",
    "        \"\"\"\n",
    "        Check whether the indices of two dataframes overlap\n",
    "\n",
    "        Args:\n",
    "            df1 (pd.DataFrame): First dataframe\n",
    "            df2 (pd.DataFrame): Second dataframe\n",
    "\n",
    "        Returns:\n",
    "            bool: Whether the indices overlap in more then 1% of the rows\n",
    "        \"\"\"\n",
    "        if df1.empty:\n",
    "            return True\n",
    "        if df2.empty:\n",
    "            return True\n",
    "        \n",
    "        df1_defined = df1[df1.index.get_level_values('research_stage') != 'undefined']\n",
    "        df2_defined = df2[df2.index.get_level_values('research_stage') != 'undefined']\n",
    "        \n",
    "        if df1_defined.empty:\n",
    "            return True\n",
    "        if df2_defined.empty:\n",
    "            return True\n",
    "        \n",
    "        min_cutoff = 0.01\n",
    "        \n",
    "        return df2_defined.index.isin(df1_defined.index).sum() > min(df1_defined.shape[0], df2_defined.shape[0]) * min_cutoff\n",
    "    \n",
    "    def build_table_to_field_dict(self, data, fields):\n",
    "        ''' \n",
    "        Build a dictionary of tables to fields of interest.\n",
    "        '''\n",
    "        ## pre_check for duplicated columns and overlapping indices\n",
    "        fields_of_interest_dict = dict()\n",
    "        for table_name, df in self.dfs.items():\n",
    "            fields_of_interest = df.columns.intersection(fields)\n",
    "            if self.check_indices_overlap(data, df[fields_of_interest]): \n",
    "                fields_of_interest_dict[table_name] = fields_of_interest\n",
    "        return fields_of_interest_dict\n",
    "    \n",
    "    def get_duplicated_columns(self, table_name, fields_of_interest_dict):\n",
    "        '''\n",
    "        Get duplicated fields\n",
    "        '''\n",
    "        origin_fields_in_col = fields_of_interest_dict.get(table_name, list())\n",
    "        duplicated_fields = list()\n",
    "        for k, v in fields_of_interest_dict.items():\n",
    "            if k == table_name:\n",
    "                continue\n",
    "            if len(v):\n",
    "                duplicated_fields += set(origin_fields_in_col.intersection(v))\n",
    "        return duplicated_fields\n",
    "    \n",
    "    def rename_duplicated_columns(self, df, table_name, fields, dup_fields):\n",
    "        '''\n",
    "        Rename columns in case of duplicated columns\n",
    "        '''         \n",
    "        common_dict = {field: f'{table_name}_{field}' for field in dup_fields}\n",
    "        df = df.rename(columns=common_dict)\n",
    "        fields += common_dict.values()\n",
    "        return df, fields\n",
    "    \n",
    "    def get_not_found_fields(self, fields, renamed_cols, not_merged, data):\n",
    "        renamed_fields= np.setdiff1d(fields, renamed_cols)\n",
    "        only_merged_fields= np.setdiff1d(renamed_fields, not_merged)\n",
    "        not_found = np.setdiff1d(only_merged_fields, data.columns)\n",
    "        return not_found\n",
    "        \n",
    "    def get(self, fields: Union[str,List[str]], flexible: bool=None, squeeze: bool=None, return_fields: bool=False, keep_undefined_research_stage: bool=None):\n",
    "        \"\"\"\n",
    "        Return data for the specified fields from all tables\n",
    "\n",
    "        Args:\n",
    "            fields (List[str]): Fields to return\n",
    "            flexible (bool, optional): Whether to use fuzzy matching to find fields. Defaults to None, which uses the PhenoLoader's flexible_field_search attribute.\n",
    "            squeeze (bool, optional): Whether to squeeze the output if only one field is requested. Defaults to None, which uses the PhenoLoader's squeeze attribute.\n",
    "            return_fields (bool, optional): Whether to return the list of fields that were found. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Data for the specified fields from all tables\n",
    "        \"\"\"\n",
    "        if flexible is None:\n",
    "            flexible = self.flexible_field_search\n",
    "        if squeeze is None:\n",
    "            squeeze = self.squeeze\n",
    "        if keep_undefined_research_stage is None:\n",
    "            keep_undefined_research_stage = self.keep_undefined_research_stage\n",
    "        if isinstance(fields, str):\n",
    "            fields = [fields]\n",
    "\n",
    "        matches = fields\n",
    "        if flexible:\n",
    "            # 1. searching in dictionary, so we can access bulk fields as well as tabular fields\n",
    "            # 2. keeping the given fields, so we can access fields (e.g., index levels) that are not in the dictionary\n",
    "            # 3. approximate matches will appear after exact matches\n",
    "            matches = [self.dict.index[self.dict.index.str.contains(field, case=False)].tolist()\n",
    "                       for field in fields]\n",
    "            fields = np.hstack(fields + matches)\n",
    "\n",
    "        # check whether any field points to a parent_dataframe\n",
    "        seen_fields = set()\n",
    "        parent_dict = dict()\n",
    "        if 'parent_dataframe' in self.dict.columns:\n",
    "            parent_dict = self.dict.loc[self.dict.index.isin(fields), 'parent_dataframe'].dropna()\n",
    "        fields = np.hstack([parent_dict.get(field, field) for field in fields])\n",
    "        fields = [field for field in fields if field not in seen_fields and not seen_fields.add(field)]\n",
    "\n",
    "        data = pd.DataFrame()\n",
    "        not_merged = list()\n",
    "        renamed_cols = list()\n",
    "        \n",
    "        fields_of_interest_dict = self.build_table_to_field_dict(data, fields)\n",
    "        \n",
    "        for table_name, df in self.dfs.items():\n",
    "            if 'mapping' in table_name:\n",
    "                continue\n",
    "            \n",
    "            duplicated_fields = self.get_duplicated_columns(table_name, fields_of_interest_dict)\n",
    "            df, fields = self.rename_duplicated_columns(df, table_name, fields, duplicated_fields)\n",
    "            \n",
    "            fields_in_col = df.columns.intersection(fields)\n",
    "            fields_in_index = np.setdiff1d(np.intersect1d(df.index.names, fields), data.columns)\n",
    "            \n",
    "            if len(fields_in_col) or len(fields_in_index):\n",
    "                if not self.check_indices_overlap(data, df):\n",
    "                    warnings.warn(f'No overlap between tables, this merge is not recommended. Please view tables separately.\\\n",
    "                        This warning occurred while attempting to add columns from {table_name} to the rest of the data.')\n",
    "                    \n",
    "                    not_merged += list(fields_in_col) + list(fields_in_index)\n",
    "                    continue\n",
    "            \n",
    "            index_data_df = pd.DataFrame()\n",
    "            if len(fields_in_index):\n",
    "                for field in fields_in_index:\n",
    "                    index_data_df[field] = df.index.get_level_values(field)\n",
    "                index_data_df = index_data_df.set_index(df.index)\n",
    "            \n",
    "            df_fields = pd.concat([df[fields_in_col], index_data_df], axis=1)\n",
    "            \n",
    "            if df_fields.empty:\n",
    "                continue\n",
    "            \n",
    "            if table_name == 'age_sex':\n",
    "                keep_undefined = True\n",
    "            else: \n",
    "                keep_undefined = keep_undefined_research_stage\n",
    "                \n",
    "            data = self.__concat__(\n",
    "                data, \n",
    "                df_fields, \n",
    "                keep_undefined\n",
    "                )\n",
    "            renamed_cols += duplicated_fields\n",
    "            \n",
    "        if len(data):\n",
    "            data = data.loc[:, ~data.columns.duplicated()]\n",
    "        \n",
    "        not_found = self.get_not_found_fields(fields, renamed_cols, not_merged, data)\n",
    "        if len(not_found) and not flexible:\n",
    "            if self.errors == 'raise':\n",
    "                raise KeyError(f'Fields not found: {not_found}')\n",
    "            elif self.errors == 'warn':\n",
    "                warnings.warn(f'Fields not found: {not_found}')\n",
    "        \n",
    "        data = self.replace_bulk_data_path(data, fields)\n",
    "        \n",
    "        cols_order = [field for field in fields if field in data.columns]\n",
    "\n",
    "        if squeeze and len(cols_order) == 1:\n",
    "            return data[cols_order[0]]\n",
    "\n",
    "        if return_fields:\n",
    "            return data[cols_order], np.unique(np.hstack(matches)).tolist()\n",
    "\n",
    "        return data[cols_order]\n",
    "    \n",
    "    def replace_bulk_data_path(self, data, fields):\n",
    "        if \"item_type\" in self.dict.columns:\n",
    "            # TODO: remove. For now, its backward compatible with old data dictionaries\n",
    "            bulk_fields = self.dict.loc[self.dict.index.isin(fields)].query('item_type == \"Bulk\"')\n",
    "        else: \n",
    "            bulk_field_types = self.dict_prop.loc[self.dict_prop.is_bulk == True].index.to_list()\n",
    "            bulk_fields = self.dict.loc[self.dict.index.isin(fields)].query('field_type in @bulk_field_types')\n",
    "        cols = [col for col in bulk_fields.index.to_list() if col in data.columns] \n",
    "        dataset_bulk_data_path = {k:v.format(dataset=self.dataset) for k, v in BULK_DATA_PATH.items()}\n",
    "        category_cols = self.dict.loc[self.dict.index.isin(fields)].query('pandas_dtype == \"category\"').index\n",
    "    \n",
    "        for col in category_cols: \n",
    "            data[col] = data[col].astype(str)\n",
    "\n",
    "        data[cols] = data[cols].fillna('nan').replace(dataset_bulk_data_path, regex=True)\n",
    "        for col in category_cols: \n",
    "            data[col] = data[col].astype('category')\n",
    "            \n",
    "        return data\n",
    "\n",
    "    def has_index(self, df, value):\n",
    "        return value in df.index.names\n",
    "        \n",
    "    def is_value_in_index(self, df, value, index_name):\n",
    "        if self.has_index(df, index_name):\n",
    "            return value in df.index.get_level_values(index_name)\n",
    "        return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def join_and_filter_undefined_research_stage(df1, df2):\n",
    "        df1_defined = df1[df1.index.get_level_values('research_stage') != 'undefined']\n",
    "        df2_defined = df2[df2.index.get_level_values('research_stage') != 'undefined']\n",
    "\n",
    "        return df1_defined.join(df2_defined, how='outer')\n",
    "\n",
    "    def __concat__(self, df1, df2, keep_undefined_research_stage=False):\n",
    "\n",
    "        if df1.empty:\n",
    "            return df2\n",
    "        if df2.empty:\n",
    "            return df1\n",
    "        \n",
    "        if self.is_value_in_index(df1, 'undefined', 'research_stage') and \\\n",
    "            self.is_value_in_index(df2, 'undefined', 'research_stage') and not keep_undefined_research_stage:\n",
    "        \n",
    "            warnings.warn('filtering \"undefined\" research_stage..')\n",
    "            df = self.join_and_filter_undefined_research_stage(df1, df2)\n",
    "            return df\n",
    "        \n",
    "        return df1.join(df2, how='outer')\n",
    "        \n",
    "    def __load_age_sex__(self) -> None:\n",
    "        \"\"\"\n",
    "        Add sex and compute age from birth date.\n",
    "        \"\"\"\n",
    "        age_path = os.path.join(self.__get_dataset_path__(self.age_sex_dataset), 'events.parquet')\n",
    "        align_df = self.dfs[list(self.dfs)[0]]\n",
    "\n",
    "        if ('research_stage' in align_df.columns) or ('research_stage' in align_df.index.names):\n",
    "            try:\n",
    "                age_df = pd.read_parquet(age_path)\n",
    "                self.dfs['age_sex'] = align_df.join(\n",
    "                    age_df[['age_at_research_stage', 'sex']].droplevel('array_index'))\\\n",
    "                    .rename(columns={'age_at_research_stage': 'age'})[['age', 'sex']]\n",
    "\n",
    "            except Exception as e:\n",
    "                if self.errors == 'raise':\n",
    "                    raise(e)\n",
    "                elif self.errors == 'warn':\n",
    "                    warnings.warn(f'Error joining research_stage: {e}')\n",
    "                self.dfs['age_sex'] = pd.DataFrame(index=align_df.index).assign(age=np.nan, sex=np.nan)\n",
    "\n",
    "        else:\n",
    "            # init an empty df\n",
    "            self.dfs['age_sex'] = pd.DataFrame(index=align_df.index).assign(age=np.nan, sex=np.nan)\n",
    "\n",
    "        self.fields += ['age', 'sex']\n",
    "        ind = self.dfs['age_sex'].isnull().any(axis=1)\n",
    "        if not ind.any():  # no missing values\n",
    "            return\n",
    "\n",
    "        # fill in missing values by computing age from birth date\n",
    "        try:\n",
    "            date_cols = np.array(['collection_date', 'collection_timestamp', 'sequencing_date'])\n",
    "            date = date_cols[np.isin(date_cols, align_df.columns)][0]  # prefer first match\n",
    "        except Exception as e:\n",
    "            if self.errors == 'raise':\n",
    "                raise(e)\n",
    "            elif self.errors == 'warn':\n",
    "                warnings.warn(f'No date field found')\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            ind &= align_df[date].notnull()\n",
    "        except Exception as e:\n",
    "            if self.errors == 'raise':\n",
    "                raise(e)\n",
    "            if self.errors == 'warn':\n",
    "                warnings.warn(f'Error checking date field: {e}')\n",
    "            return\n",
    "        if not ind.any():\n",
    "            return\n",
    "\n",
    "        age_df = pd.read_parquet(age_path.replace('events', 'population'))\n",
    "\n",
    "        # trying a workaround for a pandas deprecation warning\n",
    "        age_sex = self.dfs['age_sex']\n",
    "        try:\n",
    "            age_df['birth_date'] = pd.to_datetime(\n",
    "                age_df['year_of_birth'].astype(str) + '-' + age_df['month_of_birth'].astype(str))\n",
    "\n",
    "            missing_age_sex = align_df.loc[ind, [date]].join(age_df[['sex', 'birth_date']])\\\n",
    "                .assign(age=lambda x: ((x[date] - x['birth_date']).dt.days / 365.25).round(1))\\\n",
    "                [['age', 'sex']]\n",
    "            age_sex = age_sex.join(missing_age_sex, rsuffix='_miss')\n",
    "\n",
    "        except Exception as e:\n",
    "            if self.errors == 'raise':\n",
    "                print(\"Exception occurred:\\n\", traceback.format_exc())\n",
    "                raise(e)\n",
    "            elif self.errors == 'warn':\n",
    "                warnings.warn(f'Error joining on {date}: {e}')\n",
    "\n",
    "            age_sex = age_sex.join(age_df[['sex']], rsuffix='_miss').assign(age_miss=np.nan)\n",
    "\n",
    "        age_sex['age'] = age_sex['age'].fillna(age_sex['age_miss'])\n",
    "        age_sex['sex'] = age_sex['sex'].fillna(age_sex['sex_miss'])\n",
    "        self.dfs['age_sex'] = age_sex[['age', 'sex']]\n",
    "\n",
    "    def __load_dataframes__(self) -> None:\n",
    "        \"\"\"\n",
    "        Load all tables in the dataset dictionary.\n",
    "        \"\"\"\n",
    "        self.dfs = {}\n",
    "        self.fields = set()\n",
    "        for relative_location in self.dict['relative_location'].dropna().unique():\n",
    "            parquet_name = relative_location.split(os.sep)[-1]\n",
    "            internal_location = os.sep.join(relative_location.split(os.sep)[1:])\n",
    "            \n",
    "            if any([pattern in relative_location for pattern in self.skip_dfs]):\n",
    "                \n",
    "                (f'Skipping {relative_location}')\n",
    "                continue\n",
    "            df = self.__load_one_dataframe__(internal_location)\n",
    "            if df is None:\n",
    "                continue\n",
    "            table_name = parquet_name.split('.')[0]\n",
    "            self.dfs[table_name] = df\n",
    "            if not df.index.is_unique:\n",
    "                print('Warning: index is not unique for', table_name)\n",
    "            self.fields |= set(self.dfs[table_name].columns.tolist())\n",
    "        self.fields = sorted(list(self.fields))\n",
    "\n",
    "    def __load_one_dataframe__(self, relative_location: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load one dataframe.\n",
    "\n",
    "        Args:\n",
    "            relative_location (str): the location of the dataframe\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: the loaded dataframe\n",
    "        \"\"\"\n",
    "    \n",
    "        df_path = os.path.join(self.dataset_path, relative_location)\n",
    "        \n",
    "        try:\n",
    "            data =  pd.read_parquet(df_path, **self.read_parquet_kwargs)\n",
    "        except Exception as err:\n",
    "            if self.errors == 'raise':\n",
    "                warnings.warn(f'Error loading {df_path}:\\n{err}')\n",
    "                raise err\n",
    "            if self.errors == 'warn':\n",
    "                warnings.warn(f'Error loading {df_path}:\\n{err}')\n",
    "            return None\n",
    "        \n",
    "        data = transform_dataframe(data, transform_from='coding', transform_to=self.preferred_language, \n",
    "                                   dict_df=self.dict, mapping_df=self.data_codings)\n",
    "            \n",
    "        # set the order of columns according to the dictionary\n",
    "        dict_columns = self.dict.index.intersection(data.columns)\n",
    "        other_columns = data.columns.difference(self.dict.index)\n",
    "        assert (len(dict_columns) + len(other_columns)) == len(data.columns), \"something isn't right\"\n",
    "        data = data[dict_columns.tolist() + other_columns.tolist()]\n",
    "\n",
    "        before = len(data)\n",
    "        if self.unique_index:\n",
    "            data = data.loc[~data.index.duplicated()]\n",
    "        if self.valid_dates:\n",
    "            data = data.loc[data.select_dtypes(include=['datetime64[ns]']).notnull().any(axis=1)]\n",
    "        if self.valid_stage:\n",
    "            data = data.loc[data.index.get_level_values('research_stage').notnull()]\n",
    "        after = len(data)\n",
    "        if before > after:\n",
    "            print(f'Filtered {before - after} rows')\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __load_dictionary__(self) -> None:\n",
    "        \"\"\"\n",
    "        Load dataset dictionary.\n",
    "        \"\"\"\n",
    "        self.dict = pd.read_csv(self.__get_dictionary_file_path__(self.dataset))\\\n",
    "            .dropna(subset='tabular_field_name')\\\n",
    "            .set_index('tabular_field_name')\n",
    "\n",
    "        if 'bulk_dictionary' not in self.dict.columns or len(self.dict.dropna(subset='bulk_dictionary')['bulk_dictionary']) == 0:\n",
    "            return\n",
    "\n",
    "        # bulk dictionaries\n",
    "        bulk_dicts = self.dataset_path + '/metadata/' + \\\n",
    "            self.dict.dropna(subset='bulk_dictionary')['bulk_dictionary'] + '_bulk_dictionary.csv'\n",
    "        self.dict = pd.concat([self.dict] +\n",
    "            [pd.read_csv(bd).set_index('tabular_field_name').assign(parent_dataframe=tfn)\n",
    "             for tfn, bd in bulk_dicts.items()], axis=0)\n",
    "\n",
    "    def __get_file_path__(self, dataset: str, extension: str) -> str:\n",
    "        \"\"\"\n",
    "        Get the file path for a dataset and an extension.\n",
    "\n",
    "        Args:\n",
    "            dataset (str): the name of the dataset\n",
    "            extension (str): the extension of the file\n",
    "\n",
    "        Returns:\n",
    "            str: the path to the file\n",
    "        \"\"\"\n",
    "        path = os.path.join(self.dataset_path, '*.' + extension)\n",
    "        if path.startswith('s3://'):\n",
    "           return path\n",
    "        return glob(path)[0]\n",
    "\n",
    "    def __get_dictionary_file_path__(self, dataset: str) -> str:\n",
    "        \"\"\"\n",
    "        Get the file path for data dictionary.\n",
    "\n",
    "        Args:\n",
    "            dataset (str): the name of the dataset\n",
    "\n",
    "        Returns:\n",
    "            str: the path to the file\n",
    "        \"\"\"\n",
    "        path = os.path.join(self.dataset_path, 'metadata', f'{dataset}_data_dictionary.csv')\n",
    "        if path.startswith('s3://'):\n",
    "           return path\n",
    "        return glob(path)[0]\n",
    "    \n",
    "    def __get_dataset_path__(self, dataset):\n",
    "        \"\"\"\n",
    "        Get the dataset path.\n",
    "        \n",
    "        Args:\n",
    "            dataset (str): the name of the dataset\n",
    "\n",
    "        Returns:\n",
    "            str: the path to the dataset\n",
    "        \"\"\"\n",
    "        if self.cohort is not None:\n",
    "            return os.path.join(self.base_path, dataset, self.cohort)\n",
    "        return os.path.join(self.base_path, dataset)\n",
    "\n",
    "    def __add_missing_levels__(self, data: pd.DataFrame, more_levels: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extends the index levels of the given DataFrame ('data') by appending missing levels\n",
    "        found in another DataFrame ('more_levels').\n",
    "\n",
    "        This method performs a left join on the common index levels between 'data' and 'more_levels'.\n",
    "        The extra index level from 'more_levels' is appended at the end of the index levels in 'data'.\n",
    "        Rows present in 'data' are retained, and their indices are potentially extended.\n",
    "        Rows from 'more_levels' that do not exist in 'data' are not included in the output.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            The DataFrame whose index levels you want to extend.\n",
    "            This DataFrame's rows will all be present in the output DataFrame.\n",
    "\n",
    "        more_levels : pd.DataFrame\n",
    "            The DataFrame used as a reference for adding extra index levels to 'data'.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            A new DataFrame with the same rows as 'data' but potentially extended index levels.\n",
    "\n",
    "        Example:\n",
    "        --------\n",
    "        # Given the following DataFrames:\n",
    "        data: index(levels: A, B), columns: [value]\n",
    "        more_levels: index(levels: A, B, C), columns: [value]\n",
    "\n",
    "        # The output will have:\n",
    "        index(levels: A, B, C), columns: [value]\n",
    "        \"\"\"\n",
    "        # Identify common index levels\n",
    "        common_index_levels = list(set(data.index.names).intersection(set(more_levels.index.names)))\n",
    "        if len(common_index_levels) == 0:\n",
    "            return data\n",
    "        \n",
    "        # Identify the extra index level in more_levels\n",
    "        extra_index_levels = [l for l in more_levels.index.names if l not in common_index_levels]\n",
    "        \n",
    "        if not extra_index_levels:\n",
    "            # If there are no additional index levels in more_levels, return data as is\n",
    "            return data\n",
    "\n",
    "        # Reset the index of more_levels to convert all index levels to columns\n",
    "        more_levels_reset = more_levels.reset_index()\n",
    "        \n",
    "        # Select only the common and extra index levels\n",
    "        more_levels_subset = more_levels_reset[common_index_levels + extra_index_levels].drop_duplicates()\n",
    "        \n",
    "        # Merge the dataframes on the common index levels using a left join\n",
    "        new_data = pd.merge(data.reset_index(), more_levels_subset, how='left', on=common_index_levels)\n",
    "        \n",
    "        # Explicitly set the order of the new index levels to maintain\n",
    "        # the original order in 'more_levels' and append the extra remaining levels\n",
    "        new_index_order = more_levels.index.names + \\\n",
    "            [l for l in data.index.names if l not in more_levels.index.names]\n",
    "        \n",
    "        new_data.set_index(new_index_order, inplace=True)\n",
    "        \n",
    "        return new_data\n",
    "\n",
    "    def __slice_bulk_partition__(self, field_name: str, paths: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Slice the bulk partition based on the field name.\n",
    "\n",
    "        Args:\n",
    "            field_name (str): The name of the field.\n",
    "            paths (pd.Series): The paths to be sliced.\n",
    "\n",
    "        Returns:\n",
    "            pd.Series: The sliced paths.\n",
    "        \"\"\"\n",
    "        if 'field_type' not in self.dict:\n",
    "            return paths\n",
    "\n",
    "        partition = self.dict.loc[field_name, 'field_type']\n",
    "        if isinstance(partition, pd.Series):\n",
    "            partition = partition.iloc[0]\n",
    "        if type(partition) is not str:\n",
    "            return paths\n",
    "        if 'partition' not in partition:\n",
    "            return paths\n",
    "        partition = partition.split(':')[1].strip()\n",
    "\n",
    "        paths[paths.str[-1] != '/'] += '/'\n",
    "        paths += partition + '=' + field_name + '/'\n",
    "        return paths\n",
    "\n",
    "    def __slice_bulk_data__(self, field_name: str) -> dict:\n",
    "        \"\"\"\n",
    "        Generate keyword arguments for pd.read_parquet based on the field name.\n",
    "\n",
    "        Args:\n",
    "            field_name (str): The name of the field.\n",
    "\n",
    "        Returns:\n",
    "            dict: The keyword arguments for pd.read_parquet.\n",
    "        \"\"\"\n",
    "        if 'field_type' not in self.dict:\n",
    "            return {}\n",
    "\n",
    "        slice_by = self.dict.loc[field_name, 'field_type']\n",
    "        if type(field_name) is str:\n",
    "            field_name = [field_name]\n",
    "        if isinstance(slice_by, pd.Series):\n",
    "            # get all fields that have the same field_type (first type)\n",
    "            field_name = slice_by[slice_by == slice_by.iloc[0]].index.tolist()\n",
    "            slice_by = slice_by.iloc[0]\n",
    "        if type(slice_by) is not str:\n",
    "            return {}\n",
    "        if 'column' in slice_by:\n",
    "            return {'columns': field_name}\n",
    "        if 'rows' in slice_by:\n",
    "            return {'filters': [[(slice_by.split(':')[1].strip(), '==', f)] for f in field_name],\n",
    "                    'engine': 'pyarrow'}\n",
    "        return {}\n",
    "\n",
    "    def describe_field(self, fields: Union[str,List[str]], return_summary: bool=False):\n",
    "        \"\"\"\n",
    "        Display a summary dataframe for the specified fields from all tables\n",
    "\n",
    "        Args:\n",
    "            fields (List[str]): Fields to return\n",
    "            return_summary (Bool): whether to return the summary dataframe\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Data for the specified fields from all tables\n",
    "        \"\"\"\n",
    "        if isinstance(fields, str):\n",
    "            fields = [fields]\n",
    "        \n",
    "        summary_df = pd.concat([self.dict.loc[fields,:].T,\n",
    "                                custom_describe(self[fields])])\n",
    "        display(summary_df)\n",
    "        if return_summary:\n",
    "            return summary_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the dataset name to load the dataset. It may contain multiple tables. Age / sex will be added to the data by default. The default `base_path` is set to work on the research platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhenoLoader for fundus with\n",
       "78 fields\n",
       "2 tables: ['fundus', 'age_sex']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl = PhenoLoader('fundus', errors='warn')\n",
    "pl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PhenoLoader class contains several usefull attributes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data dictionary of the dataset displays the description of each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field_string</th>\n",
       "      <th>description_string</th>\n",
       "      <th>parent_dataframe</th>\n",
       "      <th>relative_location</th>\n",
       "      <th>value_type</th>\n",
       "      <th>units</th>\n",
       "      <th>sampling_rate</th>\n",
       "      <th>field_type</th>\n",
       "      <th>array</th>\n",
       "      <th>cohorts</th>\n",
       "      <th>data_type</th>\n",
       "      <th>debut</th>\n",
       "      <th>pandas_dtype</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tabular_field_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fundus_image_left</th>\n",
       "      <td>Fundus image (left)</td>\n",
       "      <td>Fundus image (left)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fundus/fundus.parquet</td>\n",
       "      <td>Text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Image file (individual)</td>\n",
       "      <td>Single</td>\n",
       "      <td>10K</td>\n",
       "      <td>image</td>\n",
       "      <td>2021-02-17</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fundus_image_right</th>\n",
       "      <td>Fundus image (right)</td>\n",
       "      <td>Fundus image (right)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fundus/fundus.parquet</td>\n",
       "      <td>Text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Image file (individual)</td>\n",
       "      <td>Single</td>\n",
       "      <td>10K</td>\n",
       "      <td>image</td>\n",
       "      <td>2021-02-17</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>collection_date</th>\n",
       "      <td>Collection date (YYYY-MM-DD)</td>\n",
       "      <td>Collection date (YYYY-MM-DD)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fundus/fundus.parquet</td>\n",
       "      <td>Date</td>\n",
       "      <td>Time</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Data</td>\n",
       "      <td>Single</td>\n",
       "      <td>10K</td>\n",
       "      <td>tabular</td>\n",
       "      <td>2021-02-17</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    field_string  \\\n",
       "tabular_field_name                                 \n",
       "fundus_image_left            Fundus image (left)   \n",
       "fundus_image_right          Fundus image (right)   \n",
       "collection_date     Collection date (YYYY-MM-DD)   \n",
       "\n",
       "                              description_string  parent_dataframe  \\\n",
       "tabular_field_name                                                   \n",
       "fundus_image_left            Fundus image (left)               NaN   \n",
       "fundus_image_right          Fundus image (right)               NaN   \n",
       "collection_date     Collection date (YYYY-MM-DD)               NaN   \n",
       "\n",
       "                        relative_location value_type units  sampling_rate  \\\n",
       "tabular_field_name                                                          \n",
       "fundus_image_left   fundus/fundus.parquet      Text    NaN            NaN   \n",
       "fundus_image_right  fundus/fundus.parquet      Text    NaN            NaN   \n",
       "collection_date     fundus/fundus.parquet       Date  Time            NaN   \n",
       "\n",
       "                                 field_type   array cohorts data_type  \\\n",
       "tabular_field_name                                                      \n",
       "fundus_image_left   Image file (individual)  Single     10K     image   \n",
       "fundus_image_right  Image file (individual)  Single     10K     image   \n",
       "collection_date                        Data  Single     10K   tabular   \n",
       "\n",
       "                         debut    pandas_dtype  \n",
       "tabular_field_name                              \n",
       "fundus_image_left   2021-02-17          string  \n",
       "fundus_image_right  2021-02-17          string  \n",
       "collection_date     2021-02-17  datetime64[ns]  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.dict.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fundus', 'age_sex'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame with MultiIndex\n",
    "data = {\n",
    "    'value': [10, 20, 30, 40]\n",
    "}\n",
    "tuples = [('A', 'x'), ('A', 'y'), ('B', 'x'), ('B', 'y')]\n",
    "index = pd.MultiIndex.from_tuples(tuples, names=['outer', 'inner'])\n",
    "df = pd.DataFrame(data, index=index)\n",
    "\n",
    "# Reset the index\n",
    "df_reset = df.reset_index()\n",
    "\n",
    "# Re-add the desired index levels ('outer', 'inner' in this case) as index\n",
    "df_final = df_reset.set_index(['outer', 'inner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>fundus_image_left</th>\n",
       "      <th>fundus_image_right</th>\n",
       "      <th>collection_date</th>\n",
       "      <th>fractal_dimension_left</th>\n",
       "      <th>fractal_dimension_right</th>\n",
       "      <th>artery_average_width_left</th>\n",
       "      <th>artery_average_width_right</th>\n",
       "      <th>artery_distance_tortuosity_left</th>\n",
       "      <th>artery_distance_tortuosity_right</th>\n",
       "      <th>artery_fractal_dimension_left</th>\n",
       "      <th>...</th>\n",
       "      <th>vein_fractal_dimension_left</th>\n",
       "      <th>vein_fractal_dimension_right</th>\n",
       "      <th>vein_squared_curvature_tortuosity_left</th>\n",
       "      <th>vein_squared_curvature_tortuosity_right</th>\n",
       "      <th>vein_tortuosity_density_left</th>\n",
       "      <th>vein_tortuosity_density_right</th>\n",
       "      <th>vein_vessel_density_left</th>\n",
       "      <th>vein_vessel_density_right</th>\n",
       "      <th>vessel_density_left</th>\n",
       "      <th>vessel_density_right</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>participant_id</th>\n",
       "      <th>cohort</th>\n",
       "      <th>research_stage</th>\n",
       "      <th>array_index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>10k</th>\n",
       "      <th>00_00_visit</th>\n",
       "      <th>0</th>\n",
       "      <td>/path/to/file</td>\n",
       "      <td>/path/to/file</td>\n",
       "      <td>2022-11-16</td>\n",
       "      <td>1.564989</td>\n",
       "      <td>1.520885</td>\n",
       "      <td>18430.284751</td>\n",
       "      <td>19038.547771</td>\n",
       "      <td>3.668175</td>\n",
       "      <td>3.271147</td>\n",
       "      <td>1.355673</td>\n",
       "      <td>...</td>\n",
       "      <td>1.410553</td>\n",
       "      <td>1.403108</td>\n",
       "      <td>14.208195</td>\n",
       "      <td>6.098432</td>\n",
       "      <td>0.700187</td>\n",
       "      <td>0.698546</td>\n",
       "      <td>0.046645</td>\n",
       "      <td>0.045864</td>\n",
       "      <td>0.080377</td>\n",
       "      <td>0.078671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>10k</th>\n",
       "      <th>00_00_visit</th>\n",
       "      <th>0</th>\n",
       "      <td>/path/to/file</td>\n",
       "      <td>/path/to/file</td>\n",
       "      <td>2022-06-30</td>\n",
       "      <td>1.542311</td>\n",
       "      <td>1.534158</td>\n",
       "      <td>17315.398780</td>\n",
       "      <td>19099.489575</td>\n",
       "      <td>2.095461</td>\n",
       "      <td>1.634782</td>\n",
       "      <td>1.368933</td>\n",
       "      <td>...</td>\n",
       "      <td>1.387527</td>\n",
       "      <td>1.332864</td>\n",
       "      <td>8.999069</td>\n",
       "      <td>8.702682</td>\n",
       "      <td>0.740806</td>\n",
       "      <td>0.708911</td>\n",
       "      <td>0.037896</td>\n",
       "      <td>0.046853</td>\n",
       "      <td>0.074197</td>\n",
       "      <td>0.064578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>10k</th>\n",
       "      <th>00_00_visit</th>\n",
       "      <th>0</th>\n",
       "      <td>/path/to/file</td>\n",
       "      <td>/path/to/file</td>\n",
       "      <td>2021-10-05</td>\n",
       "      <td>1.482051</td>\n",
       "      <td>1.545097</td>\n",
       "      <td>15375.866993</td>\n",
       "      <td>19855.576862</td>\n",
       "      <td>2.776472</td>\n",
       "      <td>2.747015</td>\n",
       "      <td>1.360404</td>\n",
       "      <td>...</td>\n",
       "      <td>1.411881</td>\n",
       "      <td>1.408791</td>\n",
       "      <td>13.119227</td>\n",
       "      <td>9.936669</td>\n",
       "      <td>0.627281</td>\n",
       "      <td>0.675100</td>\n",
       "      <td>0.053022</td>\n",
       "      <td>0.048063</td>\n",
       "      <td>0.079515</td>\n",
       "      <td>0.082102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 fundus_image_left  \\\n",
       "participant_id cohort research_stage array_index                     \n",
       "0              10k    00_00_visit    0               /path/to/file   \n",
       "1              10k    00_00_visit    0               /path/to/file   \n",
       "2              10k    00_00_visit    0               /path/to/file   \n",
       "\n",
       "                                                 fundus_image_right  \\\n",
       "participant_id cohort research_stage array_index                      \n",
       "0              10k    00_00_visit    0                /path/to/file   \n",
       "1              10k    00_00_visit    0                /path/to/file   \n",
       "2              10k    00_00_visit    0                /path/to/file   \n",
       "\n",
       "                                                 collection_date  \\\n",
       "participant_id cohort research_stage array_index                   \n",
       "0              10k    00_00_visit    0                2022-11-16   \n",
       "1              10k    00_00_visit    0                2022-06-30   \n",
       "2              10k    00_00_visit    0                2021-10-05   \n",
       "\n",
       "                                                  fractal_dimension_left  \\\n",
       "participant_id cohort research_stage array_index                           \n",
       "0              10k    00_00_visit    0                          1.564989   \n",
       "1              10k    00_00_visit    0                          1.542311   \n",
       "2              10k    00_00_visit    0                          1.482051   \n",
       "\n",
       "                                                  fractal_dimension_right  \\\n",
       "participant_id cohort research_stage array_index                            \n",
       "0              10k    00_00_visit    0                           1.520885   \n",
       "1              10k    00_00_visit    0                           1.534158   \n",
       "2              10k    00_00_visit    0                           1.545097   \n",
       "\n",
       "                                                  artery_average_width_left  \\\n",
       "participant_id cohort research_stage array_index                              \n",
       "0              10k    00_00_visit    0                         18430.284751   \n",
       "1              10k    00_00_visit    0                         17315.398780   \n",
       "2              10k    00_00_visit    0                         15375.866993   \n",
       "\n",
       "                                                  artery_average_width_right  \\\n",
       "participant_id cohort research_stage array_index                               \n",
       "0              10k    00_00_visit    0                          19038.547771   \n",
       "1              10k    00_00_visit    0                          19099.489575   \n",
       "2              10k    00_00_visit    0                          19855.576862   \n",
       "\n",
       "                                                  artery_distance_tortuosity_left  \\\n",
       "participant_id cohort research_stage array_index                                    \n",
       "0              10k    00_00_visit    0                                   3.668175   \n",
       "1              10k    00_00_visit    0                                   2.095461   \n",
       "2              10k    00_00_visit    0                                   2.776472   \n",
       "\n",
       "                                                  artery_distance_tortuosity_right  \\\n",
       "participant_id cohort research_stage array_index                                     \n",
       "0              10k    00_00_visit    0                                    3.271147   \n",
       "1              10k    00_00_visit    0                                    1.634782   \n",
       "2              10k    00_00_visit    0                                    2.747015   \n",
       "\n",
       "                                                  artery_fractal_dimension_left  \\\n",
       "participant_id cohort research_stage array_index                                  \n",
       "0              10k    00_00_visit    0                                 1.355673   \n",
       "1              10k    00_00_visit    0                                 1.368933   \n",
       "2              10k    00_00_visit    0                                 1.360404   \n",
       "\n",
       "                                                  ...  \\\n",
       "participant_id cohort research_stage array_index  ...   \n",
       "0              10k    00_00_visit    0            ...   \n",
       "1              10k    00_00_visit    0            ...   \n",
       "2              10k    00_00_visit    0            ...   \n",
       "\n",
       "                                                  vein_fractal_dimension_left  \\\n",
       "participant_id cohort research_stage array_index                                \n",
       "0              10k    00_00_visit    0                               1.410553   \n",
       "1              10k    00_00_visit    0                               1.387527   \n",
       "2              10k    00_00_visit    0                               1.411881   \n",
       "\n",
       "                                                  vein_fractal_dimension_right  \\\n",
       "participant_id cohort research_stage array_index                                 \n",
       "0              10k    00_00_visit    0                                1.403108   \n",
       "1              10k    00_00_visit    0                                1.332864   \n",
       "2              10k    00_00_visit    0                                1.408791   \n",
       "\n",
       "                                                  vein_squared_curvature_tortuosity_left  \\\n",
       "participant_id cohort research_stage array_index                                           \n",
       "0              10k    00_00_visit    0                                         14.208195   \n",
       "1              10k    00_00_visit    0                                          8.999069   \n",
       "2              10k    00_00_visit    0                                         13.119227   \n",
       "\n",
       "                                                  vein_squared_curvature_tortuosity_right  \\\n",
       "participant_id cohort research_stage array_index                                            \n",
       "0              10k    00_00_visit    0                                           6.098432   \n",
       "1              10k    00_00_visit    0                                           8.702682   \n",
       "2              10k    00_00_visit    0                                           9.936669   \n",
       "\n",
       "                                                  vein_tortuosity_density_left  \\\n",
       "participant_id cohort research_stage array_index                                 \n",
       "0              10k    00_00_visit    0                                0.700187   \n",
       "1              10k    00_00_visit    0                                0.740806   \n",
       "2              10k    00_00_visit    0                                0.627281   \n",
       "\n",
       "                                                  vein_tortuosity_density_right  \\\n",
       "participant_id cohort research_stage array_index                                  \n",
       "0              10k    00_00_visit    0                                 0.698546   \n",
       "1              10k    00_00_visit    0                                 0.708911   \n",
       "2              10k    00_00_visit    0                                 0.675100   \n",
       "\n",
       "                                                  vein_vessel_density_left  \\\n",
       "participant_id cohort research_stage array_index                             \n",
       "0              10k    00_00_visit    0                            0.046645   \n",
       "1              10k    00_00_visit    0                            0.037896   \n",
       "2              10k    00_00_visit    0                            0.053022   \n",
       "\n",
       "                                                  vein_vessel_density_right  \\\n",
       "participant_id cohort research_stage array_index                              \n",
       "0              10k    00_00_visit    0                             0.045864   \n",
       "1              10k    00_00_visit    0                             0.046853   \n",
       "2              10k    00_00_visit    0                             0.048063   \n",
       "\n",
       "                                                  vessel_density_left  \\\n",
       "participant_id cohort research_stage array_index                        \n",
       "0              10k    00_00_visit    0                       0.080377   \n",
       "1              10k    00_00_visit    0                       0.074197   \n",
       "2              10k    00_00_visit    0                       0.079515   \n",
       "\n",
       "                                                 vessel_density_right  \n",
       "participant_id cohort research_stage array_index                       \n",
       "0              10k    00_00_visit    0                       0.078671  \n",
       "1              10k    00_00_visit    0                       0.064578  \n",
       "2              10k    00_00_visit    0                       0.082102  \n",
       "\n",
       "[3 rows x 76 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.dfs['fundus'].head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All availbale fields (columns) in all tables can be listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artery_average_width_left',\n",
       " 'artery_average_width_right',\n",
       " 'artery_distance_tortuosity_left',\n",
       " 'artery_distance_tortuosity_right',\n",
       " 'artery_fractal_dimension_left']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.fields[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>vein_average_width_right</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>participant_id</th>\n",
       "      <th>cohort</th>\n",
       "      <th>research_stage</th>\n",
       "      <th>array_index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>10k</th>\n",
       "      <th>00_00_visit</th>\n",
       "      <th>0</th>\n",
       "      <td>18436.428634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>10k</th>\n",
       "      <th>00_00_visit</th>\n",
       "      <th>0</th>\n",
       "      <td>18888.160314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>10k</th>\n",
       "      <th>00_00_visit</th>\n",
       "      <th>0</th>\n",
       "      <td>19013.865043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>10k</th>\n",
       "      <th>00_00_visit</th>\n",
       "      <th>0</th>\n",
       "      <td>18809.012493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>10k</th>\n",
       "      <th>00_00_visit</th>\n",
       "      <th>0</th>\n",
       "      <td>19428.986690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  vein_average_width_right\n",
       "participant_id cohort research_stage array_index                          \n",
       "0              10k    00_00_visit    0                        18436.428634\n",
       "1              10k    00_00_visit    0                        18888.160314\n",
       "2              10k    00_00_visit    0                        19013.865043\n",
       "3              10k    00_00_visit    0                        18809.012493\n",
       "4              10k    00_00_visit    0                        19428.986690"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl['vein_average_width_right']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access any of the fields (e.g., `vein_average_width_right`, `age`) or indices (e.g., `research_stage`) from any of the tables via the data loader API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>research_stage</th>\n",
       "      <th>vein_average_width_right</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>participant_id</th>\n",
       "      <th>cohort</th>\n",
       "      <th>research_stage</th>\n",
       "      <th>array_index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>10k</th>\n",
       "      <th>00_00_visit</th>\n",
       "      <th>0</th>\n",
       "      <td>00_00_visit</td>\n",
       "      <td>18436.428634</td>\n",
       "      <td>43.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>10k</th>\n",
       "      <th>00_00_visit</th>\n",
       "      <th>0</th>\n",
       "      <td>00_00_visit</td>\n",
       "      <td>18888.160314</td>\n",
       "      <td>53.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>10k</th>\n",
       "      <th>00_00_visit</th>\n",
       "      <th>0</th>\n",
       "      <td>00_00_visit</td>\n",
       "      <td>19013.865043</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>10k</th>\n",
       "      <th>00_00_visit</th>\n",
       "      <th>0</th>\n",
       "      <td>00_00_visit</td>\n",
       "      <td>18809.012493</td>\n",
       "      <td>44.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>10k</th>\n",
       "      <th>00_00_visit</th>\n",
       "      <th>0</th>\n",
       "      <td>00_00_visit</td>\n",
       "      <td>19428.986690</td>\n",
       "      <td>50.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 research_stage  \\\n",
       "participant_id cohort research_stage array_index                  \n",
       "0              10k    00_00_visit    0              00_00_visit   \n",
       "1              10k    00_00_visit    0              00_00_visit   \n",
       "2              10k    00_00_visit    0              00_00_visit   \n",
       "3              10k    00_00_visit    0              00_00_visit   \n",
       "4              10k    00_00_visit    0              00_00_visit   \n",
       "\n",
       "                                                  vein_average_width_right  \\\n",
       "participant_id cohort research_stage array_index                             \n",
       "0              10k    00_00_visit    0                        18436.428634   \n",
       "1              10k    00_00_visit    0                        18888.160314   \n",
       "2              10k    00_00_visit    0                        19013.865043   \n",
       "3              10k    00_00_visit    0                        18809.012493   \n",
       "4              10k    00_00_visit    0                        19428.986690   \n",
       "\n",
       "                                                   age  sex  \n",
       "participant_id cohort research_stage array_index             \n",
       "0              10k    00_00_visit    0            43.5    0  \n",
       "1              10k    00_00_visit    0            53.7    1  \n",
       "2              10k    00_00_visit    0            26.2    0  \n",
       "3              10k    00_00_visit    0            44.6    1  \n",
       "4              10k    00_00_visit    0            50.3    0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl[['research_stage', 'vein_average_width_right', 'age', 'sex']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access time series or bulk data that is stored separately for each sample via the data loader API. In the following example, the data loader retrieves the relative path of each sample's bulk file from the main table (where it is stored in the field `fundus_image_left`), converts it to an absolute path, and loads the file. This is repeated for 2 samples and returned as a list. In the case of parquet DataFrames, there is no need to define the `load_func` and multiple DFs are concatenated by deafult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "participant_id  cohort  research_stage  array_index\n",
       "0               10k     00_00_visit     0              /path/to/file\n",
       "1               10k     00_00_visit     0              /path/to/file\n",
       "2               10k     00_00_visit     0              /path/to/file\n",
       "3               10k     00_00_visit     0              /path/to/file\n",
       "4               10k     00_00_visit     0              /path/to/file\n",
       "Name: fundus_image_left, dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.dfs['fundus']['fundus_image_left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "pl.dfs['fundus']['fundus_image_left'] = [f'M0/images/fundus_{i}.png' for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl.load_bulk_data('fundus_image_left', participant_id=[0, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can perform flexible field search (with regex support), when initializing the PhenoLoader as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = PhenoLoader('fundus', flexible_field_search=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the following command will search for any field starting with \"fractal\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>fractal_dimension_left</th>\n",
       "      <th>fractal_dimension_right</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>participant_id</th>\n",
       "      <th>cohort</th>\n",
       "      <th>research_stage</th>\n",
       "      <th>array_index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>10k</th>\n",
       "      <th>00_00_visit</th>\n",
       "      <th>0</th>\n",
       "      <td>1.564989</td>\n",
       "      <td>1.520885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>10k</th>\n",
       "      <th>00_00_visit</th>\n",
       "      <th>0</th>\n",
       "      <td>1.542311</td>\n",
       "      <td>1.534158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>10k</th>\n",
       "      <th>00_00_visit</th>\n",
       "      <th>0</th>\n",
       "      <td>1.482051</td>\n",
       "      <td>1.545097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>10k</th>\n",
       "      <th>00_00_visit</th>\n",
       "      <th>0</th>\n",
       "      <td>1.548773</td>\n",
       "      <td>1.539352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>10k</th>\n",
       "      <th>00_00_visit</th>\n",
       "      <th>0</th>\n",
       "      <td>1.554922</td>\n",
       "      <td>1.557029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  fractal_dimension_left  \\\n",
       "participant_id cohort research_stage array_index                           \n",
       "0              10k    00_00_visit    0                          1.564989   \n",
       "1              10k    00_00_visit    0                          1.542311   \n",
       "2              10k    00_00_visit    0                          1.482051   \n",
       "3              10k    00_00_visit    0                          1.548773   \n",
       "4              10k    00_00_visit    0                          1.554922   \n",
       "\n",
       "                                                  fractal_dimension_right  \n",
       "participant_id cohort research_stage array_index                           \n",
       "0              10k    00_00_visit    0                           1.520885  \n",
       "1              10k    00_00_visit    0                           1.534158  \n",
       "2              10k    00_00_visit    0                           1.545097  \n",
       "3              10k    00_00_visit    0                           1.539352  \n",
       "4              10k    00_00_visit    0                           1.557029  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl['^fractal']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can summarize a field or set of fields by the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fundus_image_right</th>\n",
       "      <th>collection_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>field_string</th>\n",
       "      <td>Fundus image (right)</td>\n",
       "      <td>Collection date (YYYY-MM-DD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>description_string</th>\n",
       "      <td>Fundus image (right)</td>\n",
       "      <td>Collection date (YYYY-MM-DD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parent_dataframe</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relative_location</th>\n",
       "      <td>fundus/fundus.parquet</td>\n",
       "      <td>fundus/fundus.parquet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>value_type</th>\n",
       "      <td>Text</td>\n",
       "      <td>Date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>units</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sampling_rate</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>field_type</th>\n",
       "      <td>Image file (individual)</td>\n",
       "      <td>Data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>array</th>\n",
       "      <td>Single</td>\n",
       "      <td>Single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cohorts</th>\n",
       "      <td>10K</td>\n",
       "      <td>10K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_type</th>\n",
       "      <td>image</td>\n",
       "      <td>tabular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>debut</th>\n",
       "      <td>2021-02-17</td>\n",
       "      <td>2021-02-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pandas_dtype</th>\n",
       "      <td>string</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>most_frequent</th>\n",
       "      <td>/path/to/file</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-10-05 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-11-16 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         fundus_image_right               collection_date\n",
       "field_string           Fundus image (right)  Collection date (YYYY-MM-DD)\n",
       "description_string     Fundus image (right)  Collection date (YYYY-MM-DD)\n",
       "parent_dataframe                        NaN                           NaN\n",
       "relative_location     fundus/fundus.parquet         fundus/fundus.parquet\n",
       "value_type                            Text                           Date\n",
       "units                                   NaN                          Time\n",
       "sampling_rate                           NaN                           NaN\n",
       "field_type          Image file (individual)                          Data\n",
       "array                                Single                        Single\n",
       "cohorts                                 10K                           10K\n",
       "data_type                             image                       tabular\n",
       "debut                            2021-02-17                    2021-02-17\n",
       "pandas_dtype                         string                datetime64[ns]\n",
       "count                                     5                             5\n",
       "unique                                    1                             5\n",
       "most_frequent                 /path/to/file                           NaN\n",
       "min                                     NaN           2021-10-05 00:00:00\n",
       "max                                     NaN           2022-11-16 00:00:00\n",
       "mean                                    NaN                           NaN\n",
       "median                                  NaN                           NaN\n",
       "std                                     NaN                           NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl.describe_field(['fundus_image_right', 'collection_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
